[Deep Forgetting  & Underlearning for Safely Scoped LLMs](https://www.alignmentforum.org/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms)

Fine-tuning can rapidly undo safety training.
## Scoping latent capabilities
- Passive: only allows specific types of tasks on which the model is trained for.
  $\Rightarrow$ Incapable of anything else.
- Active: makes the model forget only specific undesired properties.

---

[[Methods to Remove Knowledge from AI Model]]
[[What to scope out?]]


Interesting for [[Gametheory of AI Alignment]]