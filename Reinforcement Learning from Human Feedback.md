
**Variations of RLHF:**
- Human Feedback → AI Feedback
- Policy → possible supervised fine-tuning on curated data.
- Reward Models → possible supervised direct preference optimization (DPO).

**Perspective:**

RLHF is not a solution for aligning superhuman systems, but it might provide the basis for safer systems. Apply iteratively (IDA):

Human $\to$ AI v.1 $\to$ AI v.2 $\to$ ...  $\to$ AI v.n 

At each step we apply RLHF.


